<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cornell RPAL</title>
    <link>https://rpal.cs.cornell.edu/index.xml</link>
    <description>Recent content on Cornell RPAL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Nov 2017 23:47:43 -0500</lastBuildDate>
    <atom:link href="https://rpal.cs.cornell.edu/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Paper Accepted to HRI 2018</title>
      <link>https://rpal.cs.cornell.edu/news/hri2018/</link>
      <pubDate>Thu, 23 Nov 2017 23:47:43 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/hri2018/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Social Momentum: A Framework for Legible Navigation in Dynamic Multi-Agent Environments&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/chris/&#34;&gt;Christoforos Mavrogiannis&lt;/a&gt;, &lt;a href=&#34;https://rpal.cs.cornell.edu/people/wil/&#34;&gt;Wil Thomason&lt;/a&gt;, and &lt;a href=&#34;https://rpal.cs.cornell.edu/people/ross/&#34;&gt;Ross Knepper&lt;/a&gt;, has been accepted to &lt;a href=&#34;http://humanrobotinteraction.org/2018/&#34;&gt;&lt;strong&gt;HRI 2018&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;From the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Intent-expressive robot motion has been shown to result in increased efficiency and reduced
planning efforts for human or robot partners in implicitly or explicitly collaborative
scenarios. Existing frameworks for generating intent-expressive robot behaviors have typically
focused on applications in static or structured environments. Under such settings, emphasis is
placed towards communicating the robot&amp;rsquo;s intended final configuration to other agents. However,
in dynamic, unstructured and multi-agent domains, such as pedestrian environments, knowledge of
the robot&amp;rsquo;s final configuration is not sufficiently informative, as it completely ignores the
complex dynamics of interaction among agents. To address this problem, we focus on the
generation of motion that communicates an agent&amp;rsquo;s intention toward a socially compliant
collision avoidance strategy rather than its destination. We contribute a planning framework
that estimates the intended avoidance strategies of others, superimposes them, and generates an
expressive, socially compliant robot action that reinforces the expectations of others regarding
the scene evolution. This action facilitates inference and decision making for everyone, as
illustrated in the simplified topological pattern of agents&amp;rsquo; trajectories at the end of the
execution. Extensive simulations demonstrate that our framework consistently achieves
significantly lower topological complexity, compared against common benchmark approaches in the
area of multi-agent collision avoidance. The significance of this result for real world
applications is demonstrated by a user study that revealed statistical evidence suggesting that
multi-agent trajectories of lower topological complexity tend to enable observers to infer the
intentions of actors more quickly and more accurately.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Paper Accepted to IROS 2017</title>
      <link>https://rpal.cs.cornell.edu/news/iros2017_paper/</link>
      <pubDate>Sat, 29 Jul 2017 23:47:43 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/iros2017_paper/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Socially Competent Navigation Planning by Deep Learning of
Multi-Agent Path Topologies&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/chris/&#34;&gt;Christoforos Mavrogiannis&lt;/a&gt;, &lt;a href=&#34;https://rpal.cs.cornell.edu/people/valts/&#34;&gt;Valts
Blukis&lt;/a&gt;, and &lt;a href=&#34;https://rpal.cs.cornell.edu/people/ross/&#34;&gt;Ross Knepper&lt;/a&gt; has been
accepted to &lt;strong&gt;&lt;a href=&#34;http://iros2017.org&#34;&gt;IROS 2017&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;From the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We present a novel, data-driven framework for planning socially competent robot behaviors in
crowded environments. The core of our approach is a topological model of collective navigation
behaviors, based on braid groups. This model constitutes the basis for the design of a
human-inspired probabilistic inference mechanism that predicts the topology of multiple agentsâ€™
future trajectories, given observations of the context. We derive an approximation of this
mechanism by employing a neural network learning architecture on synthetic data of collective
navigation behaviors. Our planner makes use of this mechanism as a tool for interpreting the
context and understanding what future behaviors are in compliance with it. The planning agent
makes use of this understanding to determine a personal action that contributes to the context
in the most clear way possible, while ensuring progress to its destination. Our simulations
provide evidence that our planning framework results in socially competent navigation behaviors
not only for the planning agent, but also for interacting naive agents. Performance benefits
include (1) early conflict resolutions and (2) faster uncertainty decrease for the other agents
in the scene.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Ph.D. Student Wil Thomason Receives NSF GRFP and NDSEG Fellowship</title>
      <link>https://rpal.cs.cornell.edu/news/thomason_grfp/</link>
      <pubDate>Tue, 11 Apr 2017 17:25:09 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/thomason_grfp/</guid>
      <description>&lt;p&gt;Second-year Ph.D. student &lt;a href=&#34;https://rpal.cs.cornell.edu/people/wil/&#34;&gt;Wil Thomason&lt;/a&gt; was awarded a four-year
fellowship through the &lt;a href=&#34;https://ndseg.asee.org/&#34;&gt;National Defense Science and Engineering Graduate
Fellowship&lt;/a&gt;. He also received a three-year fellowship through the National
Science Foundation&amp;rsquo;s &lt;a href=&#34;https://www.nsf.gov/news/news_summ.jsp?cntn_id=191361&amp;amp;org=NSF&amp;amp;from=news&#34;&gt;Graduate Research Fellowship
Program&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Claire Liang</title>
      <link>https://rpal.cs.cornell.edu/people/claire/</link>
      <pubDate>Tue, 07 Mar 2017 17:50:12 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/claire/</guid>
      <description>&lt;p&gt;Claire is a first-year PhD student in computer science working on the Hanabi Implicature Project. In this work she
is developing a game AI that she believes matches human intuition&amp;rsquo;s use of implicature and is
exploring the impact of quantity of presented information in the game&amp;rsquo;s user interface.  She has
previously worked on stem cell population simulation including spatial modeling, and protein
crystallizability prediction. Her current interests lie in mathematics - specifically geometry and
topology - and its use in a broad range of real world modeling questions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Nominated for Best Paper Award at HRI 2017</title>
      <link>https://rpal.cs.cornell.edu/news/hri2017_nominated_best_paper/</link>
      <pubDate>Tue, 07 Mar 2017 17:25:09 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/hri2017_nominated_best_paper/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Implicit Communication in a Joint Action&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/ross/&#34;&gt;Ross Knepper&lt;/a&gt;,
&lt;a href=&#34;https://rpal.cs.cornell.edu/people/chris/&#34;&gt;Christoforos Mavrogiannis&lt;/a&gt;, &lt;a href=&#34;https://rpal.cs.cornell.edu/people/julia/&#34;&gt;Julia Proft&lt;/a&gt;,
and &lt;a href=&#34;https://rpal.cs.cornell.edu/people/claire/&#34;&gt;Claire Liang&lt;/a&gt;, has been nominated for a &lt;strong&gt;Best Paper Award&lt;/strong&gt; at
&lt;a href=&#34;http://humanrobotinteraction.org/2017/&#34;&gt;&lt;strong&gt;HRI 2017&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For more on the paper, please see &lt;a href=&#34;https://rpal.cs.cornell.edu/news/hri2017/&#34;&gt;the announcement of its acceptance&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zach Zweig Vinegar</title>
      <link>https://rpal.cs.cornell.edu/people/zach/</link>
      <pubDate>Tue, 07 Feb 2017 05:56:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/zach/</guid>
      <description>&lt;p&gt;I was a computer science major in the College of Engineering at Cornell University. My current research interests lie in the areas of human-robot interaction, computer vision, and augmented reality. I am also a member of Cornell&amp;rsquo;s Robotic Personal Assistants Lab run by Prof. Ross Knepper. Currently, I am working on a telepresence robot equipped with a 360 degree RGB-D sensor. The goal is to use the data coming from the sensor to enable the robot to seamlessly and smoothly navigate crowded pedestrian environments. More details about me can be found on my website.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Valts Blukis</title>
      <link>https://rpal.cs.cornell.edu/people/valts/</link>
      <pubDate>Mon, 06 Feb 2017 09:58:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/valts/</guid>
      <description>&lt;p&gt;Valts is a second-year PhD student in computer science. He graduated with a degree in Electrical and
Electronic Engineering from Nanyang Technological University in Singapore. In the past he has worked
on rescue robotics and stereo vision. He is interested in robotics and machine learning, in
particular developing algorithms that would improve robot understanding of the real world and allow
competent interactions with people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Navigation</title>
      <link>https://rpal.cs.cornell.edu/projects/social_nav/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:33 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/social_nav/</guid>
      <description>&lt;p&gt;Despite the great progress in the field of robotic navigation over the past few
decades, navigating a human environment remains a hard task for a robot, due to
the lack of formal rules guiding traffic, the lack of explicit communication
among agents and the unpredictability of human behavior. Existing approaches
often result in robot motion that is hard to read, which causes unpredictable
human reactions to which the robot in turn reacts to, contributing to an
oscillatory joint behavior that hinders humansâ€™ paths. We argue that the root of
the problem lies in the failure from the robotâ€™s part to convey consistently its
intentions to human observers.&lt;/p&gt;

&lt;p&gt;This project aims at developing an autonomous robotic system, capable of
navigating crowded environments in a socially competent fashion. To this end, we
develop novel models, algorithms, software and systems, which we plan on
validating experimentally in real-world scenarios.&lt;/p&gt;

&lt;p&gt;The main novelty of our approach lies in the development of a novel planning
framework for closed-loop navigation in dynamic multi-agent workspaces. The core
of the framework is a novel topological representation, based on braid groups,
that models the collective behavior of multiple agents. Based on this
representation and employing data-driven techniques, our algorithms generate
motion plans that are consistent with the perceived context, thus resulting in
socially competent robot behaviors that allow for smooth integration of mobile
robots in crowded human environments. Our framework is inspired by insights from
studies on pedestrian behavior and action interpretation and leverages the power
of implicit communication to overcome the complications of the uncertainties
induced by the imperfections of existing models of human decision making.&lt;/p&gt;

&lt;p&gt;Our robot platform is a Suitable Technologies Beam Pro, equipped with on-board
computation and an Occam Omni Stereo 360 RGBD camera, interfaced through ROS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Situated Gesture and Speech</title>
      <link>https://rpal.cs.cornell.edu/data/gestures/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:23 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/data/gestures/</guid>
      <description>

&lt;p&gt;As a part of our work on &lt;a href=&#34;https://rpal.cs.cornell.edu/projects/gestures/&#34;&gt;unfamiliar gesture recognition&lt;/a&gt;, we
encountered a need for a large, high-quality dataset of &lt;strong&gt;situated gesture and speech&lt;/strong&gt; &amp;mdash; gestures
and speech used at the same time by the same person to describe the same things. Such a dataset is
central to our approach to zero-shot learning for gesture understanding (&lt;a href=&#34;https://www.cs.cornell.edu/~wil/papers/iser2016_unfamiliargestures.pdf&#34;&gt;Thomason and Knepper,
2016&lt;/a&gt;). As we were unable to
find a large dataset of this type, we collected this dataset during 2016 and 2017.&lt;/p&gt;

&lt;h1 id=&#34;dataset-collection&#34;&gt;Dataset Collection&lt;/h1&gt;

&lt;p&gt;The data in this set were collected by recording participants in an experiment designed to elicit a
high volume of coincident gesture and speech. Participants were given a set of instructions for
folding a moderately complex piece of origami and told that the instructions had been generated by a
machine learning model. They were then asked to use the instructions to teach the study conductor
how to fold the piece of origami without ever showing the instructions to the study conductor.&lt;/p&gt;

&lt;p&gt;To avoid inducing a bias toward unnatural gesture use, participants were never told to use gesture.
Instead, the study conductor told participants that any mode of communication except for showing the
instructions or looking at what had been folded thus far was acceptable.&lt;/p&gt;

&lt;p&gt;However, due to the construction of the origami instructions, participants found it very difficult
to complete the exercise (and convey the instructions to the study conductor) using speech alone.
Thus, participants resorted to simultaneous speech and gesture to describe the geometry of the
origami and the folding actions which it required.&lt;/p&gt;

&lt;h1 id=&#34;dataset-properties&#34;&gt;Dataset Properties&lt;/h1&gt;

&lt;p&gt;This dataset currently comprises &lt;strong&gt;26 trials&lt;/strong&gt; of roughly &lt;strong&gt;20 minutes&lt;/strong&gt; each of recorded data. We
have an additional 12 trials which we are still processing for addition to the dataset. NiTE
skeleton data and audio are provided for each trial. Some trials include raw depth data, but this is
unfortunately not available for all trials.&lt;/p&gt;

&lt;p&gt;Data are currently provided in raw format (described below). In the coming months, we will release a
pre-processed version of the data with recordings segmented into individual gestures.&lt;/p&gt;

&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&amp;ldquo;Fold the two sides together like this&amp;rdquo;&lt;/strong&gt;
&lt;img src=&#34;https://rpal.cs.cornell.edu/img/gesture1.png&#34; alt=&#34;Example 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&amp;ldquo;Grab the corner and pull apart&amp;rdquo;&lt;/strong&gt;
&lt;img src=&#34;https://rpal.cs.cornell.edu/img/gesture2.png&#34; alt=&#34;Example 2&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;data-download&#34;&gt;Data Download&lt;/h1&gt;

&lt;p&gt;You can download various forms of the dataset below.&lt;/p&gt;

&lt;h2 id=&#34;raw-data&#34;&gt;Raw Data&lt;/h2&gt;

&lt;p&gt;These are the raw data from the collection experiment, unprocessed except to remove noise. They are
contained in one &lt;code&gt;tar.gz&lt;/code&gt; archive containing a separate directory for each trial. Audio files are
stored as &lt;code&gt;.flac&lt;/code&gt; files, and skeleton data is saved as pickled Python objects in the format returned
by the NiTE framework.&lt;/p&gt;

&lt;p&gt;You can download the raw data &lt;a href=&#34;https://rpal.cs.cornell.edu/data/gestures/raw.tar.gz&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;data-tools&#34;&gt;Data Tools&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Visualization:&lt;/strong&gt;
&lt;a href=&#34;https://github.com/Cornell-RPAL/nite-skeleton-visualizer&#34;&gt;nite-skeleton-visualizer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gesture Segmentation:&lt;/strong&gt; Forthcoming!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Audio Alignment:&lt;/strong&gt; Forthcoming!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unfamiliar Gestures</title>
      <link>https://rpal.cs.cornell.edu/projects/gestures/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:23 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/gestures/</guid>
      <description>&lt;p&gt;Human communication is highly multimodal, including speech, gesture, gaze, facial
expressions, and body language. Robots serving as human teammates must act on such
multimodal communicative inputs from humans, even when the message may not be
clear from any single modality. In this paper, we explore a method for achieving increased
understanding of complex, situated communications by leveraging coordinated
natural language, gesture, and context.&lt;/p&gt;

&lt;p&gt;Our work departs from the traditional model of gesture recognition in that the set of gestures it
can recognize is not limited to the gestural lexicon used for its training. Even in simplified
domains, naive classifiers can fail to recognize instances of trained gestures due to human
gestural variability. Humans resort to gesture when speech is insufficient, such as due to
inability to recall a word, inability to be heard, or inadequate time to formulate speech.
For these reasons, gesture is prevalent in human discourse. Yet gestures defy attempts
at canonical classification both due to variations within and among individuals and due
to their subjective interpretations. We define the unfamiliar gesture understanding
problem: given an observation of a previously unseen gesture (i.e. a gesture of a class
not present in any training data given to the system), we wish to output a contextually
reasonable description in natural language of the gestureâ€™s intended meaning.&lt;/p&gt;

&lt;p&gt;This problem is an instance of the machine learning problem of zero-shot learning,
a burgeoning area of machine learning that seeks to classify data without having seen
examples of its class in the training stage. Most prior work in the area makes
use of a multimodal dataset to perform the zero-shot task. However, the zero-shot task
has not yet been demonstrated for gestural data. In the related one-shot learning task,
gesture understanding has been shown from only one example of a given class in the
training stage. The primary drawback of such approaches is their reliance on
a fixed lexicon of gestures. We remove this drawback by creating a novel multimodal
embedding space using techniques from convolutional neural nets to handle variable
length gestures and allow for the description of arbitrary unfamiliar gestural data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solar Airship</title>
      <link>https://rpal.cs.cornell.edu/projects/blimp/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:13 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/blimp/</guid>
      <description>&lt;p&gt;Primary objectives are to (1) build a control system that enables persistent autonomy. (2) Develop
a power system that is capable of operating the blimp and processing hardware using attached solar
panels. (3) Develop computer vision algorithms to support research efforts, scientific observations
of natural phenomena, and path planning activity. A sensor package will be designed with
collaborators at the International Livestock Research Institute in Kenya for prediction of drought.
Other future applications include surveillance of remote territory and disaster monitoring. We hope
this project will lead to further innovation in robotic independence as well as aid important
conservation efforts.&lt;/p&gt;

&lt;p&gt;This project is a collaboration of student researchers with a student-led project team. You can find
more information about joining the project team &lt;a href=&#34;https://rpal.cs.cornell.edu/projects/blimp/splash/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ross Knepper</title>
      <link>https://rpal.cs.cornell.edu/people/ross/</link>
      <pubDate>Wed, 25 Jan 2017 16:12:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/ross/</guid>
      <description>&lt;p&gt;Ross A. Knepper is an Assistant Professor in the Department of Computer Science at Cornell University. His research focuses on the theory, algorithms, and mechanisms of automated assembly and human-robot collaboration. Previously, Ross was a Research Scientist in the Distributed Robotics Lab at MIT. Ross received his M.S and Ph.D. degrees in Robotics from Carnegie Mellon University in 2007 and 2011. Before his graduate education, Ross worked in industry at Compaq, where he designed high-performance algorithms for scalable multiprocessor systems; and also in commercialization at the National Robotics Engineering Center, where he adapted robotics technologies for customers in government and industry. Ross has served as a volunteer for Interpretation at Death Valley National Park, California.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://rpal.cs.cornell.edu/about/</link>
      <pubDate>Wed, 25 Jan 2017 00:25:59 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/about/</guid>
      <description>&lt;p&gt;The Robotic Personal Assistants Lab is focused on advancing the state of the art in human-robot interaction,
robotic manipulation, automated assembly, and multi-agent planning and coordination. We work on projects fusing
theoretical advances with empirically assessed implementations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contact Information</title>
      <link>https://rpal.cs.cornell.edu/contact/</link>
      <pubDate>Wed, 25 Jan 2017 00:25:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/contact/</guid>
      <description>&lt;div class=&#34;text-center&#34;&gt;Contact us via email at &lt;b&gt;rpal@cs.{our&amp;nbsp;university}.edu&lt;/b&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Paper Accepted to HRI 2017</title>
      <link>https://rpal.cs.cornell.edu/news/hri2017/</link>
      <pubDate>Tue, 24 Jan 2017 23:47:43 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/hri2017/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Implicit Communication in a Joint Action&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/ross/&#34;&gt;Ross Knepper&lt;/a&gt;,
&lt;a href=&#34;https://rpal.cs.cornell.edu/people/chris/&#34;&gt;Christoforos Mavrogiannis&lt;/a&gt;, &lt;a href=&#34;https://rpal.cs.cornell.edu/people/julia/&#34;&gt;Julia Proft&lt;/a&gt;,
and &lt;a href=&#34;https://rpal.cs.cornell.edu/people/claire/&#34;&gt;Claire Liang&lt;/a&gt;, has been accepted to &lt;a href=&#34;http://humanrobotinteraction.org/2017/&#34;&gt;&lt;strong&gt;HRI
2017&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;From the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Actions performed in the context of a joint activity comprise two aspects: functional and
communicative. The functional component achieves the goal of the action, whereas its
communicative component, when present, expresses some information to the actorâ€™s partners in the
joint activity. The interpretation of such communication requires leveraging information that is
public to all participants, known as common ground. Much of human communication is performed
through this implicit mechanism, and humans cannot help but infer some meaning â€” whether or not
it was intended by the actor â€” from most actions. Robots must be cognizant of how their actions
will be interpreted in context. We present a framework for robots to utilize this communicative
channel on top of normal functional actions to work more effectively with human partners. We
consider the role of the actor and the observer, both individually and jointly, in implicit
communication, as well as the effects of timing. We also show how the framework maps onto
various modes of action, including natural language and motion. We consider these modes of
action in various human-robot interaction domains, including social navigation and collaborative
assembly.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>