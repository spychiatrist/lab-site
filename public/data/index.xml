<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data on Cornell RPAL</title>
    <link>https://rpal.cs.cornell.edu/data/index.xml</link>
    <description>Recent content in Data on Cornell RPAL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jan 2017 20:09:23 -0500</lastBuildDate>
    <atom:link href="https://rpal.cs.cornell.edu/data/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Situated Gesture and Speech</title>
      <link>https://rpal.cs.cornell.edu/data/gestures/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:23 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/data/gestures/</guid>
      <description>

&lt;p&gt;As a part of our work on &lt;a href=&#34;https://rpal.cs.cornell.edu/projects/gestures/&#34;&gt;unfamiliar gesture recognition&lt;/a&gt;, we
encountered a need for a large, high-quality dataset of &lt;strong&gt;situated gesture and speech&lt;/strong&gt; &amp;mdash; gestures
and speech used at the same time by the same person to describe the same things. Such a dataset is
central to our approach to zero-shot learning for gesture understanding (&lt;a href=&#34;https://www.cs.cornell.edu/~wil/papers/iser2016_unfamiliargestures.pdf&#34;&gt;Thomason and Knepper,
2016&lt;/a&gt;). As we were unable to
find a large dataset of this type, we collected this dataset during 2016 and 2017.&lt;/p&gt;

&lt;h1 id=&#34;dataset-collection&#34;&gt;Dataset Collection&lt;/h1&gt;

&lt;p&gt;The data in this set were collected by recording participants in an experiment designed to elicit a
high volume of coincident gesture and speech. Participants were given a set of instructions for
folding a moderately complex piece of origami and told that the instructions had been generated by a
machine learning model. They were then asked to use the instructions to teach the study conductor
how to fold the piece of origami without ever showing the instructions to the study conductor.&lt;/p&gt;

&lt;p&gt;To avoid inducing a bias toward unnatural gesture use, participants were never told to use gesture.
Instead, the study conductor told participants that any mode of communication except for showing the
instructions or looking at what had been folded thus far was acceptable.&lt;/p&gt;

&lt;p&gt;However, due to the construction of the origami instructions, participants found it very difficult
to complete the exercise (and convey the instructions to the study conductor) using speech alone.
Thus, participants resorted to simultaneous speech and gesture to describe the geometry of the
origami and the folding actions which it required.&lt;/p&gt;

&lt;h1 id=&#34;dataset-properties&#34;&gt;Dataset Properties&lt;/h1&gt;

&lt;p&gt;This dataset comprises &lt;strong&gt;30 trials&lt;/strong&gt; of roughly &lt;strong&gt;20 minutes&lt;/strong&gt; each of recorded data. NiTE skeleton
data and audio are provided for each trial. Some trials include raw depth data, but this is
unfortunately not available for all trials.&lt;/p&gt;

&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&amp;ldquo;Fold the two sides together like this&amp;rdquo;&lt;/strong&gt;
&lt;img src=&#34;https://rpal.cs.cornell.edu/img/gesture1.png&#34; alt=&#34;Example 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&amp;ldquo;Grab the corner and pull apart&amp;rdquo;&lt;/strong&gt;
&lt;img src=&#34;https://rpal.cs.cornell.edu/img/gesture2.png&#34; alt=&#34;Example 2&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;data-download&#34;&gt;Data Download&lt;/h1&gt;

&lt;p&gt;You can download various forms of the dataset below.&lt;/p&gt;

&lt;h2 id=&#34;raw-data&#34;&gt;Raw Data&lt;/h2&gt;

&lt;p&gt;These are the raw data from the collection experiment, unprocessed except to remove noise. They are
contained in one &lt;code&gt;tar.gz&lt;/code&gt; archive containing a separate directory for each trial. Audio files are
stored as &lt;code&gt;.mp4&lt;/code&gt; files, and skeleton data is saved as pickled Python objects in the format returned
by the NiTE framework.&lt;/p&gt;

&lt;p&gt;You can download the raw data &lt;a href=&#34;https://rpal.cs.cornell.edu/data/gestures/raw.tar.gz&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;data-tools&#34;&gt;Data Tools&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Visualization:&lt;/strong&gt;
&lt;a href=&#34;https://github.com/Cornell-RPAL/nite-skeleton-visualizer&#34;&gt;nite-skeleton-visualizer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gesture Segmentation:&lt;/strong&gt; Forthcoming!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Audio Alignment:&lt;/strong&gt; Forthcoming!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>